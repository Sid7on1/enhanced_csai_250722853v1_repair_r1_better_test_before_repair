{
  "agent_id": "coder4",
  "task_id": "task_3",
  "files": [
    {
      "name": "memory.py",
      "purpose": "Experience replay and memory",
      "priority": "medium"
    },
    {
      "name": "reward_system.py",
      "purpose": "Reward calculation and shaping",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.AI_2507.22853v1_Repair_R1_Better_Test_Before_Repair",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.AI_2507.22853v1_Repair-R1-Better-Test-Before-Repair with content analysis. Detected project type: agent (confidence score: 10 matches).",
    "key_algorithms": [
      "Latest",
      "Reward",
      "Reasoning",
      "Iterative",
      "Reference",
      "Test",
      "Variants",
      "Repair",
      "Case",
      "Trained"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.AI_2507.22853v1_Repair-R1-Better-Test-Before-Repair.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\n1\nRepair-R1: Better Test Before Repair\nHaichuan Hu, Alibaba Cloud, 522022320050@smail.nju.edu.cn\nXiaochen Xie, Zhejiang University, xcxie@zju.edu.cn\nQuanjun Zhang, Nanjing University of Science and Technology, quanjunzhang@njust.edu.cn\nAbstract\nAPR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the\nrepairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize\ntest cases only during the inference stage, adopting an iterative approach that performs repair first and validates it\nthrough test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution\nof test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose\nRepair-R1, which introduces test cases into the model\u2019s training phase and shifts test generation to precede repair.\nThe model is required to first generate discriminative test cases that can distinguish defective behaviors, and then\nperform repair based on these tests. This enables the model to better locate defects and understand the underlying\ncauses of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone\nmodels, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on\nfour widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models,\nRepair-R1 improves repair success rate by 2.68% to 48.29%, test generation success rate by 16.38% to 53.28%, and\ntest coverage by 0.78% to 53.96%. We publish the code and weights at Github and HuggingFace.\nI. I NTRODUCTION\nAPR (Automated Program Repair) aims to automatically locate and fix potential code-related bugs, preventing\nunexpected behavior under specific inputs, thereby improving the reliability of the program. Since the emergence of\nLLMs (Large Language Models), they have been widely applied to APR. LLMs learn typical error patterns and\nrepair strategies from existing defect datasets, enabling the generalization of repair capabilities to extensive scenarios.\nHowever, existing LLM-based APR approaches primarily use buggy programs and patches during training, and\nlargely ignore test cases, treating them merely as tools for patch validation. This can lead to two main problems.\nFirst, the model becomes overly dependent on similar bugs. Through pre-training and fine-tuning, the model tends to\nmatch syntactically and semantically similar bugs from its parameterized knowledge when encountering new ones,\nattempting to resolve current issues based on past experience. Although different bugs may appear very similar in\ntext and functionality, their root causes are often quite distinct. As a result, the model may generate superficially\nsimilar patches without truly understanding or deeply analyzing the underlying defect. This is akin to relying on\nrote memorization and doing large volumes of exercises to improve exam performance\u2014an approach that often\nbackfires. The second issue is the underutilization of test data. Test cases are among the most effective tools for\nidentifying bugs, and software projects are often accompanied by a large number of unit tests to ensure code quality.arXiv:2507.22853v1  [cs.SE]  30 Jul 2025\n\n--- Page 2 ---\n2\nMoreover, during the inference phase, developers typically use error messages to guide the model in performing bug\nrepair. These facts sufficiently highlight the critical role that test cases play in the repair task. Therefore, ignoring\ntest information during the training phase of repair models represents a significant waste. Simply using test cases for\ntest-time scaling is insufficient; instead, test cases should be incorporated into the training phase to enable scaling at\nthe training stage of the repair model.\nTo address the above issues, we propose Repair-R1, which organically combines the tasks of test generation and\nAPR during the training phase. The core idea of Repair-R1 is illustrated in Figure 1, with the aim of enabling the\nmodel to first generate test cases that help locate the bug before generating patches. Specifically, such discriminative\ntest cases have two characteristics: first, they must pass the correct code, ensuring the correctness of the test cases\nthemselves; second, they should fail the buggy code, demonstrating their ability to distinguish between correct and\ndefective implementations. To implement Repair-R1, we design a novel reinforcement learning (RL) framework, as\nshown in Figure 2. Each optimization step of Repair-RL is divided into two stages: first, the test generation phase\nproduces discriminative test cases based on the bug, and then the test generation reward is computed according\nto the effectiveness ratio of the generated test cases. Next, patches are generated based on the given bug and the\ndiscriminative test cases, and its correctness is evaluated by running oracle tests, from which the code repair reward\nis calculated. Finally, the policy model is jointly optimized by combining both rewards. To enhance the effectiveness\nof the oracle tests, we also perform test augmentation.\nAlthough the correct code is used as the ground-truth patch to validate the correctness of generated test cases,\nthis utilization is implicit. The model does not directly learn the correct patches in a supervised manner, but instead\nexplores the space of possible solutions through self-play. Analogous to solving problems, the supervised learning\napproach is like a teacher directly revealing the answers, whereas Repair-R1\u2019s training is more like solving the\nproblem on one\u2019s own and then being told whether the answer is correct without being shown the details of the\nsolution. Compared to APR models trained via supervised learning, Repair-R1 demonstrates better generalization\nability. The result curves in Figure 1 also indicate that as the testing capability improves, the repair capability of\nRepair-R1 is further enhanced. We summarize our contributions as follows:\n(1) We propose Repair-R1, a novel method that utilizes reinforcement learning to train models to generate\ndiscriminative tests prior to code repair. This approach helps in locating and understanding bugs, while simultaneously\nimproving the model\u2019s abilities in both test generation and bug repair. To the best of our knowledge, Repair-R1 is\nthe first work that employs reinforcement learning to leverage test generation as a guiding mechanism for optimizing\nthe code repair process.\n(2) We validate the effectiveness of Repair-R1 under a configuration involving three models, four benchmarks,\nand five different methods (a total of 60 experiments). Experimental results show that compared to the untrained\nmodel, Repair-R1 improves repair success rate by 2.68% to 48.29%, test generation success rate by 16.38% to\n53.28%, and test coverage by 0.78% to 53.96%. Through comparative experiments and ablation studies, we also\ndemonstrate the superiority of Repair-R1 over SFT and single-objective optimization.\n(3) Repair-R1 points out a new direction for LLM-based APR, breaking away from the conventional test-driven\nrepair paradigm of \u201drepairing first and testing later,\u201d and offers a novel perspective on the repair process.\n\n--- Page 3 ---\n3\nProblem DescriptionYou are developing a user registration system and need to validate the email address entered by the user. The requirements are as follows:nThe email address must contain exactly one@symbol;nThere must be at least one character before and after the@symbol (i.e., both parts cannot be empty);nIf the conditions are not met, returnFalse; otherwise, returnTrue.Buggy Codedef check_email_format(email):if '@' in email:return Trueelse:return FalseFixed Codedef check_email_format(email):if '@' not in email:return Falseparts = email.split('@')if len(parts[0]) == 0 or len(parts[1]) == 0:return Falsereturn TrueassertTruecheck_email_format(\"alice@163.com\")Both buggy and fixed code can pass this test.\nassertTruecheck_email_format(\"163.com\")Both buggy and fixed code fail this test.\nassertTruecheck_email_format(\"@\")Buggy code passes, while fixed code fails.\nassertFalsevalid_email_format(\"a@b\u201d)Buggy code fails, while fixed code passes.\nFigure 1: This is a real-world example of bug fixing related to email format validation. To fix the bug, the APR\nmodel possibly generates four types of test cases. The first test case passes both the buggy code and the correct\ncode, and thus does not have the ability to expose the bug. The second test case fails in both the buggy and correct\ncode, making it completely incorrect and having no reference value. The third test case passes the buggy code but\nfails in the correct one \u2014 which is exactly the opposite of what is expected. The fourth test case passes the correct\ncode but fails in the buggy code, precisely covering the scenario where the bug is triggered. For the purpose of\nrepair, the fourth type of test case has the capability to distinguish between buggy and correct code, and can help\nthe model understand the bug, thereby improving the success rate of the repair. We expect that during the repair\nprocess, the model will generate as many of the fourth-type test cases as possible. To this end, we train the model\nusing RL to jointly optimize test generation and bug repair capabilities. Figures (a)-(d) reflect the simultaneous\nimprovement in test generation and bug repair performance during the model\u2019s (use Qwen2.5-Coder-1.5B-Instruct as\nan example) training and testing phases.\nII. B ACKGROUND AND RELATED WORKS\nA. Automated Program Repair\nAPR aims to assist developers in localizing and fixing program bugs automatically. Traditional APR techniques\ncan be classified as heuristic-based [ 1,2,3], constraint-based [ 4,5,6] and template-based [ 7,8]. Although effective,\ntraditional APR approaches are plagued by issues such as high computational cost and limited generalization ability.\nModern APR methods, primarily based on deep learning, have improved upon the shortcomings of previous APR\nmethods. As part of learning-based methods, Neural Machine Translation (NMT) techniques have been extensively\nstudied in recent years, e.g., TENURE [ 9], Tare [ 10], SelfAPR [ 11], RewardRepair [ 12]. They share the same insight\n\n--- Page 4 ---\n4\nthat APR can be viewed as an NMT problem that aims to translate buggy code into correct code. One downside of\nNMT-based methods is that they relatively sensitive to noise in the training dataset. To improve this, LLM-based\nmethods [ 13,14,15] leverages the general capabilities of LLMs in code-related tasks, enabling them to fix bugs\nthrough zero-shot or few-shot methods, thereby reducing the dependence on high-quality training datasets.\nHowever, most existing APR models are trained only on buggy programs and patches, ignoring the importance\nof test information. Test cases are used merely as tools to validate patch correctness during the inference phase.\nThe insufficient utilization of test cases in APR inspires us to incorporate test information into the training phase\nof APR models. We combine the test generation task with the APR task and leverage RL to jointly optimize the\nmodel\u2019s ability in both test generation and bug repair. Consequently, the model can understand and fix bugs better\nby generating discriminative test cases that reveal the root cause of bugs.\nB. Reinforcement Learning\nProximal Policy Optimization (PPO) [ 16] is one of the most popular and representative RL algorithm, which uses\nan actor-critic setup with clipped updates for stability. Different from PPO, Direct Preference Optimization (DPO)\nand its variants [ 17,18,19,20] skip the critic and directly optimize from preferences using closed-form rewards,\nimproving efficiency. Recent efficient Group Relative Policy Optimization (GRPO) [ 21] scales well with large-scale\nreinforcement learning [ 22] by optimizing policies through relative preferences among groups of actions, offering\nimproved stability and performance over traditional methods. Reinforcement learning applied specifically to coding\ntasks has also gained traction [23, 24].\nIn this paper, we adopt GRPO to optimize Repair-R1 not only for performance considerations. In addition,\nthrough mathematical derivation, we reformulate the co-optimization problem as an ELBO (Evidence Lower Bound)\nmaximization problem. We further find that, with appropriate reward design, the optimization objective of GRPO\naligns with that of ELBO, making GRPO a well-suited choice for Repair-R1.\nIII. A PPROACH\nFigure 2 illustrates the overall optimization process of Repair-R1. Before starting the optimization, we copy the\noriginal model and refer to it as the reference model, and then perform optimization on the current policy model. At\neach optimization step, we input the buggy code into the model, and first ask the policy model to generate test\ncases that can expose the bug. Based on these test cases and the bug, we then ask the policy model to generate\ncorresponding patches. The prompt is given in Figure 3. We collect the test cases and patches produced by the\npolicy model and evaluate their effectiveness. Specifically, we adopt a reinforcement learning approach to jointly\noptimize the policy model\u2019s test generation capability and code repair capability. Separate rewards are computed for\nthe policy model in terms of test generation and code repair, and a format reward is also introduced to standardize\nthe output format and syntax. Subsequently, advantages are calculated for each sample using the obtained rewards,\nand the KL divergence between the policy and reference models is computed. The policy model is then updated\nusing both the advantage and the KL divergence. Details on the reinforcement learning algorithm design and the\nreward computation can be found in Sections III-C and III-D, respectively.\n\n--- Page 5 ---\n5\nSolution Fixed by LLMs:defcount(a): count= 0  foriin a: if i== True: count+= 1+   return countPatch +10 -12 \nRequirement:First generate test cases which help discriminate the buggy code from its correct version, then return the fixed code.Bug:defcount(a): count= 0  foriin a: if i== True: count+= 1\nSolution Fixed by LLMs:defcount(a): count= 0  foriin a: -if i== True:+     if i!= True: count+= 1Patch +8 -7 \nSolution Fixed by LLMs:defcount(a): count= 0  foriin a: if i== True: count+= 1+   return count+1Patch +9 -13 \nReference ModelPolicy Model\nGenerate PatchGenerate Test\nFormat Reward\nTest Generation Reward\nCode Repair Reward<test>assert count([True,False,False]) == 1assert count([False,\u20181\u2019]) == 0</test><code>def count(a):...</code>\nOracleTestResultOn Ground Truth CodeOn Buggy Code10000Reward = 0.2Reward = 1\nReward = 0.67Input PromptCalculate RewardKL\nTest AugmentationA1A2A3A4A5A6A7A8A9AnOptimize\nGround Truth Patch:defcount(a): count= 0  foriin a: if i== True: count+= 1+   return countPatch +10 -12 \nRun OracleTestsAdvantage\nFigure 2: The Optimization Process of Repair-R1\nA. Motivation\nGiven a bug, the bug repair model will attempt to fix it based on the code-related knowledge it has acquired from\npre-trained datasets. The success of repairs may stem from one of two possibilities.\n(1) The model accurately identifies the underlying cause of the bug and applies a targeted fix.\n(2) The model recovers a similar solution by leveraging memorized patterns from the training data.\nClearly, the former case is preferable, as it reflects the model\u2019s capacity for generalization and its ability to\naddress novel and more complex bugs. Therefore, our motivation is to enhance the model\u2019s ability to understand\nand identify bugs, thereby enabling it to achieve stronger code repair capabilities.\nB. Joint Optimization of Repair and Test\nTraditional LLM-based APR is formalized as:\n\u03c0(p, b) =nY\ni=1\u03c0(pi|p<i, b) (1)\nWhere \u03c0represents the probability distribution of the repair model, brepresents the bug under repair, and p\nrepresents a n-token patch output by the model \u03c0. As discussed in the motivation section, we expect the model \u03c0to\ncomprehend the underlying cause of the bug bbefore generating the corresponding patch p. Thus, we introduce a\n\n--- Page 6 ---\n6\nSystem Prompt\nYou are an expert in the field of software testing and repairing. \nTask Prompt\nYou are given a buggy Python function, then you are supposed to first generate assertions that can\nexpose the defect, and then generate the corresponding fixed code. You may generate one or more\nassertions, return them in json list ```json```. The fixed code should also be in the format ```python```. \nOne-shot Prompt\nHere is an example. \nThe faulty function is: \n```python \ndef add (x, y): \n\"\"\" return sum of x and y \"\"\" \nreturn x - y\n``` \nAssertions that can expose the bug: \n```json \n[ \n'assert add (1, 2) == 3', \n'assert add (-1, 1) == 0', \n'assert add (-1, 2) == 1', \n'assert add (10000, 1) == 10001', \n'assert add (-1, -2) == -3'\n] ``` \nFixed code: \n```python \ndef add (x, y): \nreturn x + y \n``` \nTask Input\nNow, you are given a faulty Python function, please return: \n1. Assertions  that helps expose the bug. \n2. Fixed code  that can pass all testcases. \nThe faulty function is: \n```python\ndef fib(n: int): \n\"\"\"Return n-th Fibonacci number . \n>>> fib(10) 55 \n>>> fib(1) 1 \n>>> fib(8) 21 \n\"\"\"\n```\nFigure 3: The prompt used for optimizing Repair-R1.\nlatent variable zto represent the underlying cause of the bug. After the introduction of z, the code repair task can\nbe further formalized as follows.\n\u03c0(p,b) =Z\n\u03c0(p|z,b)\u00b7\u03c0(z|b)dz (2)\nOur goal is to optimize the model parameter \u03b8.\n\u03b8\u2217= max\n\u03b8Eb\u223cD[log\u03c0\u03b8(p, b)] (3)\nSince \u03c0\u03b8(p, b)involves an intractable integral over the latent variable z, direct computation or maximization is\nnot feasible. To address this, we introduce an approximate posterior distribution q\u03d5(z|p, b)to approximate the true\nposterior \u03c0\u03b8(z|p, b). Based on this approximation, we derive a lower bound of the log-likelihood known as ELBO,\nwhich serves as the optimization objective. ELBO is defined as follows.\nELBO =Eq\u03d5(z|p,b)[log\u03c0\u03b8(p|z, b)]\u2212KL(q\u03d5(z|p, b)\u2225\u03c0\u03b8(z|b)) (4)\n\n--- Page 7 ---\n7\nIn the field of code repair, as unit tests are commonly employed to evaluate patch correctness, identifying a test\ncase that discriminates between the correct and buggy versions effectively corresponds to pinpointing the underlying\nbug cause. Therefore, \u03c0\u03b8(z|b)is converted into a test generation task, which is optimized by both the correctness\nof generated test cases and their capabilities to discriminate between correct and buggy code. Reward design for test\ngeneration is detailed in Section III-D . Before generating patches, the model is first asked to generate such test\ncases, and then output patches based on the generated test cases and the original bug. Eq\u03d5(z|p,b)[log\u03c0\u03b8(p|z, b)]\nreflects the correctness of the generated patches, we optimize this objective by enhancing the pass rate of the patches\non the oracle test cases. Meanwhile, the ELBO optimization objective encourages KL(q\u03d5(z|p, b)\u2225\u03c0\u03b8(z|b))to be\nas small as possible, which ensures that the optimized model parameters remain close to the original ones.\nC. Reinforcement Learning Algorithm\nBy incorporating the latest model optimization techniques, we find that the GRPO (Group Relative Policy\nOptimization) algorithm aligns well with our task objectives. GRPO is used by DeepSeek-R1 and offers strong\nadvantages in balancing cost and performance. On one hand, through reward design, the objective function of GRPO\ncan encompass the optimization goals within the ELBO Formula 4 , including minimizing KL divergence and\nmaximizing patch correctness. On the other hand, GRPO is well-suited for unsupervised training, where we rely\nsolely on the test pass rate as the optimization signal. By avoiding the use of any supervised learning algorithms, we\nprevent the model from memorizing patches in the training data, leading to improved generalization performance. In\nthis study, we adopt GRPO to optimize the code repair model. GRPO aims to maximize the following objective\nfunction.\nJGRPO(\u03b8) =Ex\u223cD, y 1:G\u223c\u03c0old(\u00b7|x;C)\"\n1\nGGX\ni=11\n|yi||yi|X\nt=1min\u0010\u03c0\u03b8\u0000\nyi,t|x, yi,<t;C\u0001\n\u03c0ref\u0000\nyi,t|x, yi,<t;C\u0001\u02c6Ai,t,\nclip\u0010\u03c0\u03b8\u0000\nyi,t|x, yi,<t;C\u0001\n\u03c0ref\u0000\nyi,t|x, yi,<t;C\u0001,1\u2212\u03f5,1 +\u03f5\u0011\n\u02c6Ai,t\u0011#\n\u2212\u03b2DKL\u0002\n\u03c0\u03b8\u2225\u03c0ref\u0003\n,(5)\nWhere \u03f5and\u03b2are hyper-parameters, \u02c6Ai,tis the advantage, computed from the relative rewards of responses\nwithin each group, and yi,tistthtoken generated by the policy model. GRPO incorporates the KL divergence term\nDKLbetween the policy model and the reference model directly into the loss. During each step of training, GRPO\nfirst samples a group of responses from the policy model, then calculate rewards, advantages and KL divergence\nterm for further optimization.\nD. Reward Modeling\nIn Repair-R1, we employ rule-based rewards instead of using a reward model for the following reasons. First,\nrule-based rewards have been proven to be effective on code-related tasks. Since the correctness of a patch is\ngenerally evaluated by oracle test cases, pass rate can naturally serve as a reliable and interpretable signal for\nmeasuring patch quality. Second, rule-based rewards are more straightforward to design and debug compared to\nlearned reward models, which often require large amounts of annotated data and extensive training.\n\n--- Page 8 ---\n8\nSpecifically, we design three types of rule-based rewards: format reward, code repair reward, and test generation\nreward. The format reward ensures that the generated outputs are syntactically correct and follow the desired\nformatting conventions. The code repair reward encourages the model to produce successful and semantically\naccurate code repairs. The test generation reward promotes the generation of valid and discriminative test cases that\nare useful for evaluating the quality of patches. Equal weights are assigned to all three types of rewards.\nFormat Reward. The format reward is designed to enforce a specific output structure for the model. Specifically,\nwe introduce text-based format reward to encourage the model to generate responses in the < test > ... < /test ><\npatch > ... < /patch > format, ensuring structured outputs for both code repair and test generation. In contrast\nto DeepSeek-R1, we do not require the use of < think > ... < /think > , as we observe that excessive thinking\nsteps can lead to performance degradation in code-related tasks. In addition to the text-based reward, we incorporate\na syntax-based format reward to ensure that the model\u2019s outputs adhere to syntactically valid formats. For code,\nwe check syntactic correctness, while for test cases, it requires a structured JSON list with multiple supporting\nspecifications (e.g., assertions, input/output-style). The format reward is formalized as:\nRf(a) = \u03b1\u00b7 T(a)|{z}\nText-based reward+\u03b2\u00b7[C(acode) +T(atest)]| {z }\nSyntax-based reward(6)\nWhere arepresents the response of the model, acode represents the patch extracted from a,atestrepresents the test\ncases extracted from a,Trepresents the result of format validation, and Crepresents the result of code compiling.\nCode Repair Reward. The code repair reward assesses the effectiveness of the repair process and reflects both\nthe correctness and quality of the generated patches. Rather than treating patch correctness as a binary outcome\n(correct/incorrect), we model it as a continuous value in the range [0, 1], determined by the pass rate of oracle test\ncases. A pass rate of 100% implies full correctness, whereas a 0% pass rate indicates complete failure.\nAlthough the original datasets typically contain a sufficient number of oracle test cases (often ten or more), to\nimprove the robustness of test-based evaluation, we conduct test case augmentation. The augmented test cases are\nvalidated using manually written reference patches to ensure their correctness and exclude invalid ones. The code\nrepair reward is formalized as:\nRr(a) =1\n|T|X\nt\u2208Tf(t, a code) (7)\nWhere arepresents the response of the model, acode represents the patch extracted from a,Trepresent the\naugmented set of test cases. For each test case tinT, we evaluate acode ont, and f(t, a code)\u2208 {0,1}depends on\nwhether acode passes t. Then, the total pass rate of Tis calculated as the code repair reward Rr.\nTest Generation Reward. First, we categorize the generated test cases into two categories: valid and invalid,\nbased on their execution outcomes on both the buggy and fixed versions of the code. A valid test case satisfies\ntwo conditions. First, it is correct, with input-output behavior that matches the expected specification. Second, it is\ndiscriminative, meaning it can distinguish between the buggy and correct versions of the code, thereby helping the\nmodel locate the defect. Formally, the validity of a generated test case is defined as:\n\n--- Page 9 ---\n9\nVt=ft(t, G)\u00b7(1\u2212ft(t, B)) (8)\nWhere the validity Vtof a test tis equal to 1 if and only if the ground-truth code Gpasses t(ft(t, G)= 1) and\nthe buggy code Bfailst(ft(t, B)= 0). n all other cases, Vtis set to 0. Building upon this, the test generation\nreward further assesses the model\u2019s capability of generating multiple test cases and is formally defined as:\nRt=1\nnnX\ni=1Vi=1\nn(1\u2212F)P\u22a4(9)\nWhere vector P= [f(t1, G), f(t2, G), . . . , f (tn, G)]indicates whether each test case passes on the ground-truth\ncode G, vector F= [f(t1, B), f(t2, B), . . . , f (tn, B)]indicates whether each test case passes on the buggy code\nB, and nis the number of generated tests. If no tests is generated and n= 0,Rtis set to 0.\nIV. E XPERIMENTAL SETUP\nA. Dataset Construction\nDataset construction involves four main steps: collecting normal samples, generating defective variants, performing\ndefect validation and filtering, and train-test split.\nNormal Samples Collection. We select four widely used benchmarks for code generation, including HumanEval,\nMBPP, CodeForces, CodeContests. For HumanEval and MBPP, we included all available samples. In the case of\nCodeContests and CodeForces, we execute the ground truth solutions on the oracle tests and filter out samples with\na runtime of less than 3 seconds.\nDefective Variants Generation. For all collected samples, we use GPT-4o as the mutation model and require it\nto generate at least 10 defective versions for each sample.\nDefect Validation and Filtering. To ensure the validity and distinctiveness of the generated mutants, we first run\nthe oracle test on each mutant to verify that the defect could be detected (i.e., killed by the test). Based on the\nresults, we further remove semantically redundant mutants to maintain the quality and diversity of the dataset.\nTrain-test Split. Finally, the filtered defect dataset was partitioned into training and test sets at a 4:1 ratio,\nwith care taken to ensure that no original sample appeared in both sets, thereby preventing data contamination.\nSpecifically, the training set contains 5,421 samples, and the test set contains 1,358 samples.\nB. Evaluation Metrics\nWe evaluate Repair-R1 on the following three metrics:\n\u2022Bugfix : Bug fix rate.\n\u2022Test: Success rate of generating test cases that can both pass the ground truth patch and fail the buggy code.\n\u2022Tcov : The proportion of bugs covered by at least one test that can pass the ground truth patch and fail the\nbuggy code.\n\n--- Page 10 ---\n10\nV. E XPERIMENT RESULTS\nAs shown in Table I, we evaluate three Qwen models with varying sizes and architectures across four defect\nbenchmarks. For each benchmark, we conduct five experimental configurations: (1) using the vanilla model without\nany adaptation, marked as Vanilla; (2) fine-tuning on defect-repair pairs, marked as SFT; (3) applying reinforcement\nlearning solely on the repair task, marked as RL-Repair; (4) applying reinforcement learning solely on the test\ngeneration task, marked as RL-Test; and (5) employing collaborative reinforcement learning on both tasks in a joint\nmanner, marked as RL-Both. In the following sections, we present an analysis of the experimental results.\nTable I: The performance of Repair-R1 (RL-Both) is assessed across four widely-used benchmarks, including\nHumanEval, MBPP, CodeForces and CodeContests.\nModelHumanEval (112 bugs) MBPP (257 bugs) CodeForces (585 bugs) CodeContests (404 bugs)\nBugfix Test Tcov Bugfix Test Tcov Bugfix Test Tcov Bugfix Test Tcov\nQwen2.5-Coder-1.5B-Instruct (Vanilla) 33.04% 17.34% 37.50% 23.74% 9.58% 23.74% 4.79% 7.99% 14.36% 5.45% 1.67% 2.72%\nQwen2.5-Coder-1.5B-Instruct (SFT) 23.21% 3.93% 8.04% 9.34% 2.09% 5.84% 16.75% 7.08% 9.57% 24.01% 1.69% 2.48%\nQwen2.5-Coder-1.5B-Instruct (RL-Test) 33.93% 48.23% 73.21% 28.40% 28.99% 58.37% 3.25% 39.57% 51.45% 3.71% 50.84% 52.48%\nQwen2.5-Coder-1.5B-Instruct (RL-Repair) 75.00% 37.54% 52.71% 62.65% 25.48% 32.37% 44.10% 31.59% 38.72% 41.83% 35.85% 45.43%\nQwen2.5-Coder-1.5B-Instruct (RL-Both) 81.25% 48.07% 71.43% 66.15% 29.00% 43.58% 46.67% 40.76% 50.09% 39.85% 54.95% 56.68%\nQwen2.5-Coder-3B-Instruct (Vanilla) 51.79% 24.45% 56.25% 38.91% 18.54% 44.36% 10.26% 9.89% 29.91% 13.12% 5.64% 16.09%\nQwen2.5-Coder-3B-Instruct (SFT) 19.53% 6.43% 14.06% 22.57% 6.68% 13.54% 29.02% 6.22% 15.03% 33.05% 6.10% 12.53%\nQwen2.5-Coder-3B-Instruct (RL-Test) 56.25% 48.59% 59.82% 46.30% 40.70% 46.69% 14.70% 40.77% 61.54% 12.87% 55.24% 58.42%\nQwen2.5-Coder-3B-Instruct (RL-Code) 79.46% 26.74% 58.93% 68.87% 20.04% 48.25% 52.31% 16.68% 41.37% 47.77% 8.71% 23.02%\nQwen2.5-Coder-3B-Instruct (RL-Both) 85.71% 50.10% 60.71% 69.65% 36.58% 45.14% 53.85% 41.74% 63.76% 50.00% 55.22% 59.16%\nQwen3-4B (Vanilla) 83.93% 40.00% 81.25% 48.64% 26.18% 53.31% 35.21% 27.52% 57.26% 34.16% 11.93% 31.44%\nQwen3-4B (SFT) 66.07% 8.22% 16.96% 67.70% 12.69% 28.40% 54.70% 4.00% 5.47% 45.30% 1.79% 3.47%\nQwen3-4B (RL-Test) 71.43% 57.70% 77.68% 58.37% 50.11% 70.04% 31.28% 44.55% 68.38% 30.94% 56.86% 62.38%\nQwen3-4B (RL-Code) 84.82% 40.50% 86.61% 70.82% 26.03% 61.87% 51.79% 30.90% 64.62% 51.49% 14.99% 40.10%\nQwen3-4B (RL-Both) 86.61% 59.49% 73.21% 71.98% 55.27% 73.15% 54.19% 43.90% 66.67% 51.73% 58.11% 61.39%\nA. Test Generation Helps Repair Better\nThrough RL training, the repair capability of Repair-R1 is significantly improved compared to the vanilla model,\nand the test generation ability is also enhanced accordingly. As shown in Table I, all three models demonstrate\nconsistent improvements in both repair and test generation across the four benchmarks. The repair success rate\nincreases by 2.68% to 48.29%, the test generation success rate improves by 16.38% to 53.28%, and the test coverage\nis enhanced by 0.78% to 53.96%.\n4.9%\n21.4%\n1.8%\n72.0%Qwen2.5-Coder-1.5B(Vanilla)\nfix w/ test\nfix w/o test\nfail w/ test\nfail w/o test\n39.0%\n32.4%13.6%15.0%Qwen2.5-Coder-1.5B(RL)\nfix w/ test\nfix w/o test\nfail w/ test\nfail w/o test\n11.0%\n26.6%\n2.8%59.7%Qwen2.5-Coder-3B(Vanilla)\nfix w/ test\nfix w/o test\nfail w/ test\nfail w/o test\n47.3%\n29.4%11.3%12.0%Qwen2.5-Coder-3B(RL)\nfix w/ test\nfix w/o test\nfail w/ test\nfail w/o test\n8.0%\n58.7%2.1%31.1%Qwen-4B(Vanilla)\nfix w/ test\nfix w/o test\nfail w/ test\nfail w/o test\n55.7%\n22.5%11.2%10.6%Qwen-4B(RL)\nfix w/ test\nfix w/o test\nfail w/ test\nfail w/o test\nFigure 4: A comparison of test generation and repair correlation before and after RL\n\n--- Page 11 ---\n11\nTo further explore whether enhancements in test generation can facilitate improvements in repair capability, we\nclassify the combinations of repair outcomes and test generation results into four categories:\n(1) Successful repair with effective test case generation, marked as fix w/ test .\n(2) Successful repair without effective test case generation, marked as fix w/o test .\n(3) Failed repair with effective test case generation, marked as fail w/ test .\n(4) Failed repair without effective test case generation, marked as fail w/o test .\nAs shown in Figure 4, we compare the distribution of four types of samples across the three models before and\nafter RL training. We observe that for the vanilla models, the ability to generate tests is highly limited, which in\nturn constrains the effectiveness of repair. Most successful repairs are accompanied by failed test cases and generally\nexhibit low success rates, indicating that the model merely outputs syntactically correct code without understanding\nthe underlying cause of the defect. In contrast, after RL training, both test generation and repair capabilities are\nsignificantly improved. Successful repairs are often associated with correctly generated test cases, suggesting that\nthe model can now perform more effective repairs based on a better understanding of the defect location and its\nroot cause.\nB. Repair-R1 Outperforms SFT on Imbalanced Datasets\nTo broadly evaluate the generalization capability of Repair-R1 across different types of bugs, we pay special\nattention to the type of samples when selecting benchmarks. Specifically, HumanEval and MBPP consist of function-\nlevel bugs without entry points, while CodeContests and CodeForces use standard input and output formats. Moreover,\nthe dataset exhibits an imbalanced distribution, with HumanEval and MBPP contributing only a small portion\n(approximately one-third) of the total samples.\nThe results in Table I indicate that the disparities in data types and the imbalance in distribution have notably\nimpacted the effectiveness of SFT. For the more prevalent benchmarks, CodeForces and CodeContests, the model\nshows improved repair performance after SFT. Specifically, Qwen2.5-Coder-1.5B-Instruct improves the repair success\nrate by 11.96% and 18.56%, Qwen2.5-Coder-3B-Instruct by 18.76% and 19.93%, and Qwen-4B by 19.49% and\n11.14% on CodeForces and CodeContests, respectively. However, for the less-represented benchmarks, HumanEval\nand MBPP, the model exhibits a forgetting effect after fine-tuning, with a significant drop in repair performance\ncompared to before training. Specifically, Qwen2.5-Coder-1.5B-Instruct decrease repair success rate by 9.83% and\n14.4% and Qwen2.5-Coder-3B-Instruct by 32.26% and 16.34% on HumanEval and MBPP, respectively.\nIn contrast, RL avoids this issue. The model trained with RL demonstrates consistently improved repair performance\nacross all benchmarks. The improvement in repair success rate across the four benchmarks ranges from 2.68% to\n48.29% for the three models. By comparing SFT and RL, we find that SFT merely fits the data distribution during\ntraining. Since pre-training tasks typically do not involve code repair, significant parameter updates occur when the\nmodel is fine-tuned on defect datasets, leading it to overfit to the dominant distributions (e.g., CodeContests and\nCodeForces), while suffering from forgetting on underrepresented datasets (e.g., HumanEval). In contrast, RL builds\nupon the model\u2019s foundational coding capabilities and incrementally learns repair patterns, thereby enhancing repair\nperformance without causing catastrophic forgetting.\n\n--- Page 12 ---\n12\nC. Ablation Study\nWe conduct an ablation study on the tasks of test generation and code repair. In the RL training process, we\nretain only the Code Repair Reward or the Test Generation Reward separately, and evaluate the performance of the\ntrained model on both test generation and code repair tasks.\nAs illustrated in Table I, training the model exclusively on test generation appears to result in a marginal\nimprovement in repair capability. This observation is supported by certain results. For instance, Qwen2.5-Coder-3B-\nInstruct (Test) achieves an increase in repair success rates of 4.46%, 7.39%, and 4.44% on HumanEval, MBPP,\nand CodeForces, respectively, compared to the vanilla model. Nevertheless, this improvement is not stable, with\nQwen-4B (Test) exhibiting a decrease in repair effectiveness on HumanEval, CodeForces, and CodeContests.\nOn the other hand, models trained solely on the repair task also show improvements in test generation. This trend\nis stable, with all three models achieving varying degrees of improvement in test coverage and test effectiveness\nacross four benchmarks.\nBuilding upon these findings, we compare the two ablation models with Repair-R1 and observe that Repair-R1\ndemonstrates the most favorable performance in terms of both repair and test generation. Specifically, compared to\nRL-Repair, Repair-R1 achieves a repair success rate improvement ranging from 0.24% to 6.25%, with improvements\nobserved in 11 out of 12 comparison settings. Regarding test generation, Repair-R1 achieves a comparable level of\ntest coverage as RL-Test, while demonstrating a higher success rate in test generation.\nD. Test-time Scaling Performance of Repair-R1\nWe analyze the test-time scaling ability of Repair-R1 across different benchmarks and models. As shown in\nFigure 5, we present the repair success rate of Repair-R1 under different sampling configurations, with sample sizes\nvarying from 1 to 8.\nIn general, Qwen-4B exhibits the best scaling capability among the three models, achieving superior performance\non HumanEval and CodeForces, comparable results with Qwen2.5-Coder-3B-Instruct on CodeContests, and slightly\nlower performance on MBPP. We believe this is not entirely due to the difference in parameter scale. We observe that\nwhen the sampling size is less than or equal to 4, Qwen-4B performs slightly worse than Qwen2.5-Coder-3B-Instruct\noverall, as the latter is a code-specific language model (CodeLM) and thus has a direct advantage on code-related\ntasks. However, as the sampling size increases, the repair performance of Qwen-4B gradually surpasses that of\nQwen2.5-Coder-3B-Instruct. This can be attributed to the fact that Qwen-4B is a reasoning model, which benefits\nmore from larger sampling sizes by leveraging diverse reasoning paths to generate higher-quality patches. This trend\nalso suggests that while specialized models like Qwen2.5-Coder-3B-Instruct and Qwen2.5-Coder-1.5B-Instruct may\nhave an edge in low-sampling scenarios due to their task-specific training, general-purpose reasoning models such\nas Qwen-4B can catch up and even outperform them when given more samples, thanks to their broader knowledge\nbase and stronger reasoning capabilities.\n\n--- Page 13 ---\n13\n1 2 3 4 5 6 7 8\nBugfix@K0.820.840.860.880.900.920.940.960.98\nRepair-R1(Qwen2.5-Coder-1.5B-Instruct)\nRepair-R1(Qwen2.5-Coder-3B-Instruct)\nRepair-R1(Qwen3-4B)\n(a) HumanEval\n1 2 3 4 5 6 7 8\nBugfix@K0.700.750.800.85\nRepair-R1(Qwen2.5-Coder-1.5B-Instruct)\nRepair-R1(Qwen2.5-Coder-3B-Instruct)\nRepair-R1(Qwen3-4B) (b) MBPP\n1 2 3 4 5 6 7 8\nBugfix@K0.500.550.600.650.70\nRepair-R1(Qwen2.5-Coder-1.5B-Instruct)\nRepair-R1(Qwen2.5-Coder-3B-Instruct)\nRepair-R1(Qwen3-4B) (c) CodeForces\n1 2 3 4 5 6 7 8\nBugfix@K0.400.450.500.550.600.650.700.75\nRepair-R1(Qwen2.5-Coder-1.5B-Instruct)\nRepair-R1(Qwen2.5-Coder-3B-Instruct)\nRepair-R1(Qwen3-4B) (d) CodeContests\nFigure 5: Bugfix@K performance of Repair-R1 on different benchmarks.\nVI. C ONCLUSION\nThis paper presents Repair-R1, a method that leverages the generation of discriminative test cases to guide the\nmodel in performing bug repair, thus jointly improving its abilities in both test generation and bug repair. We design\nreward functions for both test generation and code repair, and employ the GRPO algorithm to optimize the model.\nExperimental results show that compared to vanilla models, Repair-R1 improves repair success rate by 2.68% to\n48.29%, test generation success rate by 16.38% to 53.28%, and test coverage by 0.78% to 53.96%. Comparative\nexperiments and ablation studies also demonstrate the superiority of Repair-R1 over SFT and single-objective\noptimization.\nREFERENCES\n[1]C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, \u201cGenprog: A generic method for automatic software\nrepair,\u201d Ieee transactions on software engineering , vol. 38, no. 1, pp. 54\u201372, 2011.\n[2]M. Martinez and M. Monperrus, \u201cAstor: A program repair library for java,\u201d in Proceedings of the 25th\ninternational symposium on software testing and analysis , 2016, pp. 441\u2013444.\n[3]Y . Yuan and W. Banzhaf, \u201cArja: Automated repair of java programs via multi-objective genetic programming,\u201d\nIEEE Transactions on software engineering , vol. 46, no. 10, pp. 1040\u20131067, 2018.\n[4]T. Durieux and M. Monperrus, \u201cDynamoth: dynamic code synthesis for automatic program repair,\u201d in Proceedings\nof the 11th International Workshop on Automation of Software Test , 2016, pp. 85\u201391.\n[5]M. Martinez and M. Monperrus, \u201cUltra-large repair search space with automatically mined templates: The\ncardumen mode of astor,\u201d in International symposium on search based software engineering . Springer, 2018,\npp. 65\u201386.\n[6]S. Mechtaev, J. Yi, and A. Roychoudhury, \u201cAngelix: Scalable multiline program patch synthesis via symbolic\nanalysis,\u201d in Proceedings of the 38th international conference on software engineering , 2016, pp. 691\u2013701.\n[7]K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyand \u00b4e, \u201cTbar: Revisiting template-based automated program repair,\u201d\ninProceedings of the 28th ACM SIGSOFT international symposium on software testing and analysis , 2019, pp.\n31\u201342.\n\n--- Page 14 ---\n14\n[8]Q. Zhang, C. Fang, T. Zhang, B. Yu, W. Sun, and Z. Chen, \u201cGamma: Revisiting template-based automated\nprogram repair via mask prediction,\u201d in 2023 38th IEEE/ACM International Conference on Automated Software\nEngineering (ASE) . IEEE, 2023, pp. 535\u2013547.\n[9]X. Meng, X. Wang, H. Zhang, H. Sun, X. Liu, and C. Hu, \u201cTemplate-based neural program repair,\u201d in 2023\nIEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE, 2023, pp. 1456\u20131468.\n[10] Q. Zhu, Z. Sun, W. Zhang, Y . Xiong, and L. Zhang, \u201cTare: Type-aware neural program repair,\u201d in 2023\nIEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE, 2023, pp. 1443\u20131455.\n[11] H. Ye, M. Martinez, X. Luo, T. Zhang, and M. Monperrus, \u201cSelfapr: Self-supervised program repair with test\nexecution diagnostics,\u201d in Proceedings of the 37th IEEE/ACM International Conference on Automated Software\nEngineering , 2022, pp. 1\u201313.\n[12] H. Ye, M. Martinez, and M. Monperrus, \u201cNeural program repair with execution-based backpropagation,\u201d in\nProceedings of the 44th international conference on software engineering , 2022, pp. 1506\u20131518.\n[13] I. Bouzenia, P. Devanbu, and M. Pradel, \u201cRepairagent: An autonomous, llm-based agent for program repair,\u201d\narXiv preprint arXiv:2403.17134 , 2024.\n[14] F. Zubair, M. Al-Hitmi, and C. Catal, \u201cThe use of large language models for program repair,\u201d Computer\nStandards & Interfaces , vol. 93, p. 103951, 2025.\n[15] B. Yang, H. Tian, W. Pian, H. Yu, H. Wang, J. Klein, T. F. Bissyand \u00b4e, and S. Jin, \u201cCref: An llm-based\nconversational software repair framework for programming tutors,\u201d in Proceedings of the 33rd ACM SIGSOFT\nInternational Symposium on Software Testing and Analysis , 2024, pp. 882\u2013894.\n[16] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \u201cProximal policy optimization algorithms,\u201d\n2017. [Online]. Available: https://arxiv.org/abs/1707.06347\n[17] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, \u201cDirect preference optimization:\nYour language model is secretly a reward model,\u201d Advances in neural information processing systems , vol. 36,\npp. 53 728\u201353 741, 2023.\n[18] Y . Liu, J. Ding, and X. Liu, \u201cIpo: Interior-point policy optimization under constraints,\u201d in Proceedings of the\nAAAI conference on artificial intelligence , vol. 34, no. 04, 2020, pp. 4940\u20134947.\n[19] S. Cen, J. Mei, K. Goshvadi, H. Dai, T. Yang, S. Yang, D. Schuurmans, Y . Chi, and B. Dai, \u201cValue-incentivized\npreference optimization: A unified approach to online and offline rlhf,\u201d arXiv preprint arXiv:2405.19320 , 2024.\n[20] Y . Meng, M. Xia, and D. Chen, \u201cSimpo: Simple preference optimization with a reference-free reward,\u201d Advances\nin Neural Information Processing Systems , vol. 37, pp. 124 198\u2013124 235, 2024.\n[21] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y . Li, Y . Wu et al. , \u201cDeepseekmath:\nPushing the limits of mathematical reasoning in open language models,\u201d arXiv preprint arXiv:2402.03300 ,\n2024.\n[22] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al. , \u201cDeepseek-r1:\nIncentivizing reasoning capability in llms via reinforcement learning,\u201d arXiv preprint arXiv:2501.12948 , 2025.\n[23] S. Dou, Y . Liu, H. Jia, L. Xiong, E. Zhou, W. Shen, J. Shan, C. Huang, X. Wang, X. Fan et al. , \u201cStepcoder:\nImprove code generation with reinforcement learning from compiler feedback,\u201d arXiv preprint arXiv:2402.01391 ,\n\n--- Page 15 ---\n15\n2024.\n[24] J. Li, Y . Zhao, Y . Li, G. Li, and Z. Jin, \u201cAcecoder: An effective prompting technique specialized in code\ngeneration,\u201d ACM Transactions on Software Engineering and Methodology , vol. 33, no. 8, pp. 1\u201326, 2024.",
  "project_dir": "artifacts/projects/enhanced_cs.AI_2507.22853v1_Repair_R1_Better_Test_Before_Repair",
  "communication_dir": "artifacts/projects/enhanced_cs.AI_2507.22853v1_Repair_R1_Better_Test_Before_Repair/.agent_comm",
  "assigned_at": "2025-07-31T22:30:38.805639",
  "status": "assigned"
}